{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"LightGBM_NYSE.ipynb","provenance":[],"collapsed_sections":["xx39P4x9NsgG","f8OR4dtsLNzj","66b4kq26N7gW","JEC2Nat-PxAc","gNE_MJpfTCpD","_B17pnkMWBSr"],"authorship_tag":"ABX9TyNCUaySzHKbtRzzpuLvrU6R"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"DGzn_Vz7gLm0"},"source":["# このノートブックは？\n","* CreateInitialData.ipynbで作成したデータを利用し、特徴量作成、LightGBMでの学習、予測結果のファイル出力を行う\n","* 全データを対象に学習し、予測結果のうちNYSE分のみをこの後に利用"]},{"cell_type":"markdown","metadata":{"id":"xx39P4x9NsgG"},"source":["## Google Driveのマウント"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9mi4pP5OIqj_","executionInfo":{"status":"ok","timestamp":1639375693306,"user_tz":-540,"elapsed":34169,"user":{"displayName":"Keisuke Shoji","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10321049352155053308"}},"outputId":"7d9836c3-d746-44e2-e55d-a96938ef8c5c"},"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","%cd /content/drive/'My Drive'/'PROBSPACE'/'StockPricePrediction'"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/content/drive/My Drive/PROBSPACE/StockPricePrediction\n"]}]},{"cell_type":"markdown","metadata":{"id":"f8OR4dtsLNzj"},"source":["## ライブラリのインストール、インポート"]},{"cell_type":"code","metadata":{"id":"IK_XvkEHeHh0","executionInfo":{"status":"ok","timestamp":1639375704995,"user_tz":-540,"elapsed":11693,"user":{"displayName":"Keisuke Shoji","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10321049352155053308"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"b64c44af-439e-4b41-a4a9-16b02243ddd1"},"source":["import src.install_libraries\n","import category_encoders as ce\n","from src.common_functions import *\n","from src.create_features import *\n","import lightgbm as lgb"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting category_encoders\n","  Downloading category_encoders-2.3.0-py2.py3-none-any.whl (82 kB)\n","Requirement already satisfied: patsy>=0.5.1 in /usr/local/lib/python3.7/dist-packages (from category_encoders) (0.5.2)\n","Requirement already satisfied: statsmodels>=0.9.0 in /usr/local/lib/python3.7/dist-packages (from category_encoders) (0.10.2)\n","Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.7/dist-packages (from category_encoders) (1.0.1)\n","Requirement already satisfied: pandas>=0.21.1 in /usr/local/lib/python3.7/dist-packages (from category_encoders) (1.1.5)\n","Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from category_encoders) (1.4.1)\n","Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from category_encoders) (1.19.5)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.21.1->category_encoders) (2.8.2)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.21.1->category_encoders) (2018.9)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from patsy>=0.5.1->category_encoders) (1.15.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20.0->category_encoders) (3.0.0)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20.0->category_encoders) (1.1.0)\n","Installing collected packages: category-encoders\n","Successfully installed category-encoders-2.3.0\n","Collecting ta\n","  Downloading ta-0.8.0.tar.gz (24 kB)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from ta) (1.19.5)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from ta) (1.1.5)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->ta) (2.8.2)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->ta) (2018.9)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->ta) (1.15.0)\n","Building wheels for collected packages: ta\n","  Building wheel for ta (setup.py): started\n","  Building wheel for ta (setup.py): finished with status 'done'\n","  Created wheel for ta: filename=ta-0.8.0-py3-none-any.whl size=28895 sha256=41e9b2f7602298269c8aa7a7f25725fa129f1b57f589fd022bbb2d559e9d40d2\n","  Stored in directory: /root/.cache/pip/wheels/7e/da/86/65cba22446ae2ef148de2079907264ef27feecfb7f51a45e0d\n","Successfully built ta\n","Installing collected packages: ta\n","Successfully installed ta-0.8.0\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n","  import pandas.util.testing as tm\n"]}]},{"cell_type":"markdown","metadata":{"id":"66b4kq26N7gW"},"source":["## データの読み込み"]},{"cell_type":"code","metadata":{"id":"-1T_7c2Vb1hR"},"source":["price_df2, company_df2, submission_df = read_initial_data()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"A2IPxqg5ENZM"},"source":["# 乱数固定\n","SEED = 42\n","seed_everything(SEED)\n","\n","set_pandas_options()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JEC2Nat-PxAc"},"source":["## 特徴量追加"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":492},"id":"lG1hB0NhPzDs","executionInfo":{"status":"ok","timestamp":1639375741356,"user_tz":-540,"elapsed":21396,"user":{"displayName":"Keisuke Shoji","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10321049352155053308"}},"outputId":"43844383-3151-4776-8458-0684be40c24d"},"source":["def create_features(df, group_col, val_col1, val_col2, prefix):\n","  res = pd.DataFrame(index=df.index)\n","  group_df = df.groupby(group_col) if group_col else df.copy()\n","\n","  # ラグ特徴量\n","  with Timer(f'{prefix}ラグ特徴量'):\n","    lag_df = create_lags(group_df, val_col1, [1, 2, 3, 4], prefix)\n","    res = res.join(lag_df)\n","\n","  # 単純移動平均、標準偏差\n","  with Timer(f'{prefix}単純移動平均'):\n","    roll_df = create_rolls(group_df, val_col1, [1], [52], prefix)\n","    res = res.join(roll_df)\n","\n","  # # EMA GAP\n","  # with Timer(f'{prefix}EMA_GAP'):\n","  #   ema_df = create_ema_gap(df, group_df, group_col, val_col1, [13, 26], prefix)\n","  #   res = res.join(ema_df)\n","\n","  # # Return\n","  # with Timer(f'{prefix}Return'):\n","  #   return_df = create_return(group_df, val_col2, [13, 26], prefix)\n","  #   res = res.join(return_df)\n","\n","  # # HV\n","  # with Timer(f'{prefix}HV'):\n","  #   hv_df = create_hv(df, group_df, group_col, val_col2, [13, 26], prefix)\n","  #   res = res.join(hv_df)\n","\n","  # # Skew\n","  # with Timer(f'{prefix}Skew'):\n","  #   skew_df = create_skew(df, group_df, group_col, val_col2, [13, 26], prefix)\n","  #   res = res.join(skew_df)\n","\n","  # # Kurt\n","  # with Timer(f'{prefix}Kurt'):\n","  #   kurt_df = create_kurt(group_df, val_col2, [13, 26], prefix)\n","  #   res = res.join(kurt_df)\n","\n","  # # Var\n","  # with Timer(f'{prefix}Var'):\n","  #   var_df = create_var(group_df, val_col1, [13, 26], prefix)\n","  #   res = res.join(var_df)\n","\n","  # # MIN, MAX\n","  # if group_col:\n","  #   with Timer(f'{prefix}MIN, MAX'):\n","  #     min_max_df = create_min_max(group_df, val_col1, [13, 26], prefix)\n","  #     res = res.join(min_max_df)\n","\n","  # # Up, Down\n","  # if group_col:\n","  #   with Timer(f'{prefix}Up, Down'):\n","  #     up_down_df = create_up_down(df, group_col, val_col1, [13, 26], prefix)\n","  #     res = res.join(up_down_df)\n","\n","  # # ATR\n","  # if group_col:\n","  #   with Timer(f'{prefix}ATR'):\n","  #     atr_df = create_atr(group_df, val_col2, prefix)\n","  #     res = res.join(atr_df)\n","\n","  # # RSI\n","  # if group_col:\n","  #   with Timer(f'{prefix}RSI'):\n","  #     rsi_df = create_rsi(group_df, val_col2, prefix)\n","  #     res = res.join(rsi_df)\n","\n","  # # MACD\n","  # if group_col:\n","  #   with Timer(f'{prefix}MACD'):\n","  #     macd_df = create_macd(group_df, val_col2, prefix)\n","  #     res = res.join(macd_df)\n","\n","  # # BB\n","  # if group_col:\n","  #   with Timer(f'{prefix}BB'):\n","  #     bb_df = create_bb(group_df, val_col2, prefix)\n","  #     res = res.join(bb_df)\n","\n","  # # Stochastic Oscillator\n","  # if group_col:\n","  #   with Timer(f'{prefix}Stochastic Oscillator'):\n","  #     stock_df = create_stock(group_df, val_col2, prefix)\n","  #     res = res.join(stock_df)\n","\n","  # # Parabolic Stop and Reverse\n","  # if group_col:\n","  #   with Timer(f'{prefix}Parabolic Stop and Reverse'):\n","  #     parabolic_df = create_parabolic(group_df, val_col2, prefix)\n","  #     res = res.join(parabolic_df)\n","\n","  # # KST Oscillator\n","  # if group_col:\n","  #   with Timer(f'{prefix}KST Oscillator'):\n","  #     kst_df = create_kst(group_df, val_col2, prefix)\n","  #     res = res.join(kst_df)\n","\n","  return res\n","\n","\n","def add_feature(df):\n","  res = df.copy()\n","\n","  # 日付\n","  res['Year'] = res.Date.dt.year\n","  res['Month'] = res.Date.dt.month\n","  res['Day'] = res.Date.dt.day\n","  res['WeekOfYear'] = res.Date.dt.isocalendar().week.astype(int)\n","\n","  # ターゲットエンコーディング\n","  target_columns = ['Sector', 'Industry', 'List1', 'List2']\n","  ms = ce.MEstimateEncoder(cols=target_columns, m=1.0)\n","  ms_df = ms.fit_transform(res[target_columns], res.y_diff_norm)\n","  res['List'] = res.List1\n","  res.loc[:, target_columns] = ms_df.loc[:, target_columns]\n","\n","  # 各種特徴量作成\n","  res = res.join(create_features(res, 'id', 'y_diff_norm', 'y_prev', ''))\n","\n","  # 市場平均価格を求める\n","  prefix = 'Market_'\n","  tmp_market_df = res[['Date']].copy()\n","  tmp_market_df['y'] = np.expm1(res.y)\n","  market_df = tmp_market_df.groupby('Date').mean().apply(np.log1p).reset_index()\n","  market_df['y_prev'] = market_df['y'].transform(lambda x: x.shift(1).fillna(method='bfill'))\n","  market_df['y_diff'] = market_df['y'] - market_df['y_prev']\n","  market_df['y_diff_std'] = market_df['y'].std()\n","  market_df['y_diff_norm'] = market_df['y_diff'] / market_df['y_diff_std']\n","\n","  market_columns = [prefix + c if c != 'Date' else c for c in market_df.columns]\n","  market_df.columns = market_columns\n","\n","  # 市場平均の各種特徴量作成、結合\n","  market_df = market_df.join(create_features(market_df, None, 'Market_y_diff_norm', 'Market_y_prev', prefix))\n","  res = res.merge(market_df, on='Date')\n","  market_columns = market_df.drop('Date', axis=1).columns.values.tolist()\n","\n","  # 各銘柄の特徴量を相対化\n","  # シャープレシオ\n","  # with Timer(f'Sharp Ratio'):\n","  #   group_df1 = res.groupby('id')\n","  #   group_df2 = market_df\n","  #   sharp_df = sharp_ratio(group_df1, group_df2, 'y_prev', [13, 26])\n","  #   res = res.join(sharp_df)\n","\n","  # 対象列の設定\n","  replace_columns = []\n","  # replace_columns += [f'lag_{i}' for i in [1, 2, 3, 4]]\n","  # replace_columns += [f'rmean_{i}_{j}' for i in [1] for j in [52]]\n","  # replace_columns += [f'rstd_{i}_{j}' for i in [1] for j in [52]]\n","  # replace_columns += [f'EMA_GAP_{i}' for i in [13, 26]]\n","  # replace_columns += [f'Return_{i}' for i in [13, 26]]\n","  # replace_columns += [f'HV_{i}' for i in [13, 26]]\n","  # replace_columns += [f'Skew_{i}' for i in [13, 26]]\n","  # replace_columns += [f'Kurt_{i}' for i in [13, 26]]\n","  # replace_columns += [f'Var_{i}' for i in [13, 26]]\n","\n","  # 市場平均価格の特徴量に対する各銘柄の特徴量を作成\n","  replace_columns_market = [prefix + c for c in replace_columns]\n","  feature_from_market = pd.DataFrame()\n","  for c1, c2 in zip(replace_columns, replace_columns_market):\n","    feature_from_market[f'{c1}_from_Market'] = res[c1] - res[c2]\n","  res = res.drop(replace_columns+market_columns, axis=1).join(feature_from_market).join(res[market_columns])\n","\n","  # 不要な特徴量の削除\n","  drop_columns = []\n","  drop_columns += market_columns\n","  res.drop(drop_columns, axis=1, inplace=True, errors='ignore')\n","\n","  # # PCA\n","  # with Timer('PCA'):\n","  #   res = res.join(pca(res, 2))\n","\n","  # # K-Means\n","  # with Timer('K-Means'):\n","  #   res = res.join(k_means(res, 3))\n","\n","  return res\n","\n","\n","price_df3 = add_feature(price_df2)\n","price_df3"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2021-12-13 15:09:44 ラグ特徴量 実行時間: 0.17分\n","2021-12-13 15:09:49 単純移動平均 実行時間: 0.08分\n","2021-12-13 15:09:49 Market_ラグ特徴量 実行時間: 0.00分\n","2021-12-13 15:09:49 Market_単純移動平均 実行時間: 0.00分\n"]},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Date</th>\n","      <th>id</th>\n","      <th>y</th>\n","      <th>y_prev</th>\n","      <th>y_diff</th>\n","      <th>y_diff_std</th>\n","      <th>y_diff_norm</th>\n","      <th>IPOyear</th>\n","      <th>Sector</th>\n","      <th>Industry</th>\n","      <th>List1</th>\n","      <th>List2</th>\n","      <th>Year</th>\n","      <th>Month</th>\n","      <th>Day</th>\n","      <th>WeekOfYear</th>\n","      <th>List</th>\n","      <th>lag_1</th>\n","      <th>lag_2</th>\n","      <th>lag_3</th>\n","      <th>lag_4</th>\n","      <th>rmean_1_52</th>\n","      <th>rstd_1_52</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>2011-11-13</td>\n","      <td>VGSH</td>\n","      <td>4.042036</td>\n","      <td>4.042036</td>\n","      <td>0.0</td>\n","      <td>0.017516</td>\n","      <td>0.0</td>\n","      <td>NaN</td>\n","      <td>0.007648</td>\n","      <td>0.007648</td>\n","      <td>0.003676</td>\n","      <td>0.003665</td>\n","      <td>2011</td>\n","      <td>11</td>\n","      <td>13</td>\n","      <td>45</td>\n","      <td>NASDAQ</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2011-11-13</td>\n","      <td>JEF</td>\n","      <td>2.925793</td>\n","      <td>2.925793</td>\n","      <td>0.0</td>\n","      <td>0.141216</td>\n","      <td>0.0</td>\n","      <td>NaN</td>\n","      <td>0.006327</td>\n","      <td>0.003095</td>\n","      <td>0.005265</td>\n","      <td>0.005255</td>\n","      <td>2011</td>\n","      <td>11</td>\n","      <td>13</td>\n","      <td>45</td>\n","      <td>NYSE</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2011-11-13</td>\n","      <td>IVZ</td>\n","      <td>2.692657</td>\n","      <td>2.692657</td>\n","      <td>0.0</td>\n","      <td>0.215237</td>\n","      <td>0.0</td>\n","      <td>NaN</td>\n","      <td>0.006327</td>\n","      <td>0.005636</td>\n","      <td>0.005265</td>\n","      <td>0.005255</td>\n","      <td>2011</td>\n","      <td>11</td>\n","      <td>13</td>\n","      <td>45</td>\n","      <td>NYSE</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>2011-11-13</td>\n","      <td>KTCC</td>\n","      <td>1.752672</td>\n","      <td>1.752672</td>\n","      <td>0.0</td>\n","      <td>0.207799</td>\n","      <td>0.0</td>\n","      <td>1983.0</td>\n","      <td>0.003404</td>\n","      <td>0.003106</td>\n","      <td>0.003676</td>\n","      <td>0.003665</td>\n","      <td>2011</td>\n","      <td>11</td>\n","      <td>13</td>\n","      <td>45</td>\n","      <td>NASDAQ</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>2011-11-13</td>\n","      <td>FBZ</td>\n","      <td>2.695899</td>\n","      <td>2.695899</td>\n","      <td>0.0</td>\n","      <td>0.224060</td>\n","      <td>0.0</td>\n","      <td>NaN</td>\n","      <td>0.007648</td>\n","      <td>0.007648</td>\n","      <td>0.003676</td>\n","      <td>0.003665</td>\n","      <td>2011</td>\n","      <td>11</td>\n","      <td>13</td>\n","      <td>45</td>\n","      <td>NASDAQ</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>1376755</th>\n","      <td>2019-11-24</td>\n","      <td>TYG</td>\n","      <td>NaN</td>\n","      <td>2.884177</td>\n","      <td>NaN</td>\n","      <td>0.141783</td>\n","      <td>NaN</td>\n","      <td>2004.0</td>\n","      <td>0.007648</td>\n","      <td>0.007648</td>\n","      <td>0.005265</td>\n","      <td>0.005255</td>\n","      <td>2019</td>\n","      <td>11</td>\n","      <td>24</td>\n","      <td>47</td>\n","      <td>NYSE</td>\n","      <td>-0.087223</td>\n","      <td>-0.051072</td>\n","      <td>-0.207735</td>\n","      <td>0.031837</td>\n","      <td>-0.018841</td>\n","      <td>0.221444</td>\n","    </tr>\n","    <tr>\n","      <th>1376756</th>\n","      <td>2019-11-24</td>\n","      <td>VIRC</td>\n","      <td>NaN</td>\n","      <td>1.543298</td>\n","      <td>NaN</td>\n","      <td>0.264580</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>0.005102</td>\n","      <td>0.003960</td>\n","      <td>0.003676</td>\n","      <td>0.003665</td>\n","      <td>2019</td>\n","      <td>11</td>\n","      <td>24</td>\n","      <td>47</td>\n","      <td>NASDAQ</td>\n","      <td>-0.196693</td>\n","      <td>-0.120716</td>\n","      <td>0.059876</td>\n","      <td>0.037910</td>\n","      <td>-0.010175</td>\n","      <td>0.153233</td>\n","    </tr>\n","    <tr>\n","      <th>1376757</th>\n","      <td>2019-11-24</td>\n","      <td>BIS</td>\n","      <td>NaN</td>\n","      <td>2.754241</td>\n","      <td>NaN</td>\n","      <td>1.087817</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>0.007648</td>\n","      <td>0.007648</td>\n","      <td>0.003676</td>\n","      <td>0.003665</td>\n","      <td>2019</td>\n","      <td>11</td>\n","      <td>24</td>\n","      <td>47</td>\n","      <td>NASDAQ</td>\n","      <td>-0.059202</td>\n","      <td>-0.017845</td>\n","      <td>-0.009060</td>\n","      <td>-0.071397</td>\n","      <td>-0.005512</td>\n","      <td>0.064397</td>\n","    </tr>\n","    <tr>\n","      <th>1376758</th>\n","      <td>2019-11-24</td>\n","      <td>WOOD</td>\n","      <td>NaN</td>\n","      <td>4.184112</td>\n","      <td>NaN</td>\n","      <td>0.225542</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>0.007648</td>\n","      <td>0.007648</td>\n","      <td>0.003676</td>\n","      <td>0.003665</td>\n","      <td>2019</td>\n","      <td>11</td>\n","      <td>24</td>\n","      <td>47</td>\n","      <td>NASDAQ</td>\n","      <td>-0.047620</td>\n","      <td>0.020729</td>\n","      <td>0.060736</td>\n","      <td>0.044388</td>\n","      <td>0.004269</td>\n","      <td>0.127733</td>\n","    </tr>\n","    <tr>\n","      <th>1376759</th>\n","      <td>2019-11-24</td>\n","      <td>MASI</td>\n","      <td>NaN</td>\n","      <td>5.039223</td>\n","      <td>NaN</td>\n","      <td>0.682216</td>\n","      <td>NaN</td>\n","      <td>2007.0</td>\n","      <td>0.001624</td>\n","      <td>0.004483</td>\n","      <td>0.003676</td>\n","      <td>0.003665</td>\n","      <td>2019</td>\n","      <td>11</td>\n","      <td>24</td>\n","      <td>47</td>\n","      <td>NASDAQ</td>\n","      <td>0.034399</td>\n","      <td>0.067852</td>\n","      <td>-0.045713</td>\n","      <td>0.037691</td>\n","      <td>0.011138</td>\n","      <td>0.054559</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>1376760 rows × 23 columns</p>\n","</div>"],"text/plain":["              Date    id         y    y_prev  y_diff  y_diff_std  y_diff_norm  \\\n","0       2011-11-13  VGSH  4.042036  4.042036     0.0    0.017516          0.0   \n","1       2011-11-13   JEF  2.925793  2.925793     0.0    0.141216          0.0   \n","2       2011-11-13   IVZ  2.692657  2.692657     0.0    0.215237          0.0   \n","3       2011-11-13  KTCC  1.752672  1.752672     0.0    0.207799          0.0   \n","4       2011-11-13   FBZ  2.695899  2.695899     0.0    0.224060          0.0   \n","...            ...   ...       ...       ...     ...         ...          ...   \n","1376755 2019-11-24   TYG       NaN  2.884177     NaN    0.141783          NaN   \n","1376756 2019-11-24  VIRC       NaN  1.543298     NaN    0.264580          NaN   \n","1376757 2019-11-24   BIS       NaN  2.754241     NaN    1.087817          NaN   \n","1376758 2019-11-24  WOOD       NaN  4.184112     NaN    0.225542          NaN   \n","1376759 2019-11-24  MASI       NaN  5.039223     NaN    0.682216          NaN   \n","\n","         IPOyear    Sector  Industry     List1     List2  Year  Month  Day  \\\n","0            NaN  0.007648  0.007648  0.003676  0.003665  2011     11   13   \n","1            NaN  0.006327  0.003095  0.005265  0.005255  2011     11   13   \n","2            NaN  0.006327  0.005636  0.005265  0.005255  2011     11   13   \n","3         1983.0  0.003404  0.003106  0.003676  0.003665  2011     11   13   \n","4            NaN  0.007648  0.007648  0.003676  0.003665  2011     11   13   \n","...          ...       ...       ...       ...       ...   ...    ...  ...   \n","1376755   2004.0  0.007648  0.007648  0.005265  0.005255  2019     11   24   \n","1376756      NaN  0.005102  0.003960  0.003676  0.003665  2019     11   24   \n","1376757      NaN  0.007648  0.007648  0.003676  0.003665  2019     11   24   \n","1376758      NaN  0.007648  0.007648  0.003676  0.003665  2019     11   24   \n","1376759   2007.0  0.001624  0.004483  0.003676  0.003665  2019     11   24   \n","\n","         WeekOfYear    List     lag_1     lag_2     lag_3     lag_4  \\\n","0                45  NASDAQ       NaN       NaN       NaN       NaN   \n","1                45    NYSE       NaN       NaN       NaN       NaN   \n","2                45    NYSE       NaN       NaN       NaN       NaN   \n","3                45  NASDAQ       NaN       NaN       NaN       NaN   \n","4                45  NASDAQ       NaN       NaN       NaN       NaN   \n","...             ...     ...       ...       ...       ...       ...   \n","1376755          47    NYSE -0.087223 -0.051072 -0.207735  0.031837   \n","1376756          47  NASDAQ -0.196693 -0.120716  0.059876  0.037910   \n","1376757          47  NASDAQ -0.059202 -0.017845 -0.009060 -0.071397   \n","1376758          47  NASDAQ -0.047620  0.020729  0.060736  0.044388   \n","1376759          47  NASDAQ  0.034399  0.067852 -0.045713  0.037691   \n","\n","         rmean_1_52  rstd_1_52  \n","0               NaN        NaN  \n","1               NaN        NaN  \n","2               NaN        NaN  \n","3               NaN        NaN  \n","4               NaN        NaN  \n","...             ...        ...  \n","1376755   -0.018841   0.221444  \n","1376756   -0.010175   0.153233  \n","1376757   -0.005512   0.064397  \n","1376758    0.004269   0.127733  \n","1376759    0.011138   0.054559  \n","\n","[1376760 rows x 23 columns]"]},"metadata":{},"execution_count":5}]},{"cell_type":"markdown","metadata":{"id":"gNE_MJpfTCpD"},"source":["## 学習、予測"]},{"cell_type":"code","metadata":{"id":"3qio_9OVUEzM"},"source":["lgb_params = {\n","  'objective': 'regression',\n","  'importance_type': 'gain',\n","  'metric': 'rmse',\n","  'seed': SEED,\n","  'n_jobs': -1,\n","  'verbose': -1,\n","  'n_estimators': 500, \n","  'boosting_type': 'gbdt',\n","  'colsample_bytree': 0.9556187845298308,\n","  'learning_rate': 0.10682768429326871,\n","  'min_child_samples': 230, \n","  'num_leaves': 228, \n","  'subsample': 0.9784108281460844, \n","  'subsample_freq': 2,\n","  'max_bin': 56,\n","}\n","\n","other_params = {\n","  'early_stopping_rounds': 100,\n","}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2fjME7aLENOy"},"source":["def train_evaluate(price_df, submission_df):\n","  test_size = 0.01\n","\n","  # データの分離\n","  train_df = price_df.loc[price_df.Date < Config.submission_date].reset_index(drop=True)\n","  submit_df = price_df.loc[price_df.Date == Config.submission_date].reset_index(drop=True)\n","  original_train_df = train_df.copy()\n","\n","  if test_size == 0:\n","    pass\n","  else:\n","    unique_id = train_df.id.unique()\n","    train_id, test_id = train_test_split(unique_id, test_size=test_size, random_state=SEED)\n","    train_df = train_df.loc[train_df.id.isin(train_id), :]\n","\n","  useless_cols = [\n","    'y', 'y_prev', 'y_diff', 'y_diff_std', 'y_diff_norm', 'Date', 'id',\n","    'Market_y', 'Market_y_prev', 'Market_y_diff', 'Market_y_diff_std', 'Market_y_diff_norm', 'List'\n","  ]\n","  usable_cols = train_df.columns[~train_df.columns.isin(useless_cols)]\n","  target_col = 'y_diff_norm'\n","\n","  x_train = train_df[usable_cols]\n","  y_train = train_df[target_col]\n","  x_submit = submit_df[usable_cols]\n","\n","  # 学習\n","  y_diff_std = train_df['y_diff_std']\n","  y_submit = []\n","\n","  unique_id = train_df.id.unique()\n","  kf = KFold(n_splits=5, shuffle=True, random_state=SEED)\n","  for fold, (tr_group_idx, va_group_idx) in enumerate(kf.split(unique_id)):\n","    tr_groups, va_groups = unique_id[tr_group_idx], unique_id[va_group_idx]\n","    tr_idx = train_df.id.isin(tr_groups)\n","    vl_idx = train_df.id.isin(va_groups)\n","\n","    x_tr_fold = x_train[tr_idx]\n","    y_tr_fold = y_train[tr_idx]\n","    x_vl_fold = x_train[vl_idx]\n","    y_vl_fold = y_train[vl_idx]\n","\n","    model = lgb.LGBMRegressor(**lgb_params)\n","    model.fit(\n","        x_tr_fold, y_tr_fold,\n","        eval_set=(x_vl_fold, y_vl_fold),\n","        eval_metric='rmse',\n","        verbose=False,\n","        early_stopping_rounds=other_params['early_stopping_rounds'],\n","    )\n","\n","    y_submit.append(model.predict(x_submit))\n","\n","  # submissionデータ作成\n","  submission_df['y'] = submit_df['y']\n","  submission_df['y_prev'] = submit_df['y_prev']\n","  submission_df['y_diff_std'] = submit_df['y_diff_std']\n","  submission_df['y_diff_norm'] = submit_df['y_diff_norm']\n","  submission_df['pred_y'] = np.mean(y_submit, axis=0) * submit_df['y_diff_std'].values + submit_df['y_prev'].values\n","  submission_df['exp_pred_y'] = np.expm1(submission_df['pred_y'])\n","  submission_df.loc[submission_df['exp_pred_y'] < 0, 'exp_pred_y'] = 0\n","  submission_df['pred_y_diff_norm'] = np.mean(y_submit, axis=0)\n","  submission_df['List'] = submit_df['List']\n","  submission_df['List1'] = submit_df['List1']\n","\n","  return model, submission_df\n","\n","\n","model, submission_df = train_evaluate(price_df3, submission_df)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_B17pnkMWBSr"},"source":["## submissionファイル作成"]},{"cell_type":"code","source":["submission_df2 = submission_df[['id', 'exp_pred_y']]\n","submission_df2.columns = ['id', 'y']"],"metadata":{"id":"Loe3pbFiL-Ds"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HCpSrGwjENLY"},"source":["submission_df2.to_csv(f'{Config.intermediate_dir_name}/submission_lgb_NYSE.csv', index=False)"],"execution_count":null,"outputs":[]}]}